{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c6d5a70-bd6a-4577-bb28-e52722cd8430",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### importing notebook to get Modeldata class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd1b2901-fe9c-4153-8a68-4373da44339c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001b[0m\n",
      "Collecting https://github.com/eduardhendriksen/PyForge/archive/master.tar.gz\n",
      "  Using cached https://github.com/eduardhendriksen/PyForge/archive/master.tar.gz\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting flatten_dict\n",
      "  Using cached flatten_dict-0.4.2-py2.py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: requests in /databricks/python3/lib/python3.10/site-packages (from PyForge==0.4) (2.28.1)\n",
      "Collecting requests-toolbelt\n",
      "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Requirement already satisfied: six<2.0,>=1.12 in /usr/lib/python3/dist-packages (from flatten_dict) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.10/site-packages (from requests->PyForge==0.4) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /databricks/python3/lib/python3.10/site-packages (from requests->PyForge==0.4) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.10/site-packages (from requests->PyForge==0.4) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /databricks/python3/lib/python3.10/site-packages (from requests->PyForge==0.4) (2.0.4)\n",
      "Building wheels for collected packages: PyForge\n",
      "  Building wheel for PyForge (setup.py): started\n",
      "  Building wheel for PyForge (setup.py): finished with status 'done'\n",
      "  Created wheel for PyForge: filename=PyForge-0.4-py3-none-any.whl size=23839 sha256=3572a47e1f9d9e8b90aefe8b6250812f04b8cf46bb96b10b96f0dfbd8218a7d8\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-hsjevao5/wheels/36/2b/00/cda886582b7b9dcbd63d645cbaed0ae96c3169e5945a72a4f2\n",
      "Successfully built PyForge\n",
      "Installing collected packages: flatten_dict, requests-toolbelt, PyForge\n",
      "Successfully installed PyForge-0.4 flatten_dict-0.4.2 requests-toolbelt-1.0.0\n",
      "\u001b[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%run \"./1_Modeldata\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54b52382-9ed4-463e-b79a-720a11c7cfed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#creating spark session\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"pandas_to_spark\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5350c14c-2d4c-4c43-bc49-4cbf1fe13303",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84c6e558-c31b-485c-879e-ebe410ca8bf5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"p_project_id\", \"\")\n",
    "dbutils.widgets.text(\"p_folder_id\", \"\")\n",
    "dbutils.widgets.text(\"p_model_name\", \"\")\n",
    "dbutils.widgets.text(\"p_view_name\", \"\")\n",
    "dbutils.widgets.text(\"p_project_name\",\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a25ca1e1-a76c-4d81-82a9-4bf027369d92",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Reading Parameter value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aaf654f9-042e-404d-9d88-6f5df12e482f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "v_project_id = dbutils.widgets.get(\"p_project_id\")\n",
    "v_folder_id = dbutils.widgets.get(\"p_folder_id\")\n",
    "v_model_name = dbutils.widgets.get(\"p_model_name\")\n",
    "v_view_name = dbutils.widgets.get(\"p_view_name\")\n",
    "v_project_name = dbutils.widgets.get(\"p_project_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6617c73-fd84-4e15-96d1-f95992667f4c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Initialize the Modeldata class and work with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2bddce80-485d-4028-a93c-8eb99f59492b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_data = ModelData(auth_address='https://developer.api.autodesk.com/authentication/v2/token',\n",
    "                                            client_id=\"\",\n",
    "                                            client_secret='',\n",
    "                                            scopes='data:read',\n",
    "                                            hub_id='2ee6be14-9d13-4d36-8d21-49bf9ba124cd',\n",
    "                                            project_name=v_project_name,\n",
    "                                            project_id=v_project_id,\n",
    "                                            folder_id=v_folder_id,\n",
    "                                            model_name=v_model_name,\n",
    "                                            view_name=v_view_name)\n",
    "\n",
    "model_version = model_data.version_number\n",
    "model_object_tree,model_object_properties = model_data.get_model_object_tree_properties(model_data.token, model_data.model_urn, model_data.view_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdd80f0d-8ca0-4429-b2b3-c54cb782ebfa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing tables for Walls\n",
      "Succesfully created tables for Walls\n",
      "Processing tables for Structural Columns\n",
      "Succesfully created tables for StructuralColumns\n",
      "Processing tables for Floors\n",
      "Succesfully created tables for Floors\n",
      "Processing tables for Structural Framing\n",
      "Succesfully created tables for StructuralFraming\n",
      "Processing tables for Generic Models\n",
      "Succesfully created tables for GenericModels\n",
      "Processing tables for Structural Foundations\n",
      "Succesfully created tables for StructuralFoundations\n",
      "Processing tables for Slab Edges\n",
      "Succesfully created tables for SlabEdges\n",
      "Processing tables for Columns\n",
      "Succesfully created tables for Columns\n",
      "Processing tables for Structural Connections\n",
      "Succesfully created tables for StructuralConnections\n",
      "Processing tables for Stairs\n",
      "Succesfully created tables for Stairs\n"
     ]
    }
   ],
   "source": [
    "type_names, elem_names, type_frames, elem_frames = [], [], [], []\n",
    "\n",
    "for type_name, elem_name, type_frame, elem_frame in model_data.process_object_tree(model_object_tree, model_object_properties):\n",
    "    type_names.append(type_name)\n",
    "    elem_names.append(elem_name)\n",
    "    type_frames.append(type_frame)\n",
    "    elem_frames.append(elem_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1970be1-f7f3-4310-b564-ddd5faa52f61",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Function to create Spark DataFrames\n",
    "def create_spark_dataframes(spark, frames):\n",
    "    spark_frames = [spark.createDataFrame(df) for df in frames]\n",
    "    return spark_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "331d8849-3cb3-4b65-9be1-2b3152acdd63",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create Spark DataFrames\n",
    "spark_elem_frames = create_spark_dataframes(spark, elem_frames)\n",
    "spark_type_frames = create_spark_dataframes(spark, type_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4b11f30-4489-4592-8e3c-3bf2c97a67ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dataframe_object = dict(zip(\n",
    "    [*type_names, *elem_names],\n",
    "    [*spark_type_frames, *spark_elem_frames]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7640c5b7-5eb1-47fa-9d4b-cad6807322ee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "database_name= []\n",
    "\n",
    "for i in dbutils.fs.ls('mnt/kevee/bronze/'):\n",
    "     database_name.append(i.name.split('/')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6af67f1c-5b7a-4261-86c7-53e4a248e459",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_dataframe_object(path):\n",
    "\n",
    "    dataframe_object_adls= {}\n",
    "    for i in dbutils.fs.ls(path):\n",
    "        _i = i.name.split('/')[0]\n",
    "        table_name = _i.split('.')[0]\n",
    "        dataframe_object_adls[table_name] = spark.read.format(\"parquet\").load(i.path)\n",
    "    return dataframe_object_adls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8480208c-e4be-4fdd-9b9f-b41ae02071f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if v_project_name not in database_name:\n",
    "    \n",
    "    # When creating new tables, we append them to the dataframe_object to consider all different versions of the table\n",
    "    for table, df in dataframe_object.items():\n",
    "        path = f'mnt/kevee/bronze/{v_project_name}/{table}.parquet'\n",
    "        df.write.mode('overwrite').parquet(path)\n",
    "    \n",
    "else:\n",
    "    db_path = f'mnt/kevee/bronze/{v_project_name}' \n",
    "    dataframe_object_adls = get_dataframe_object(db_path)\n",
    "            \n",
    "    for table in dataframe_object.keys():\n",
    "        if table not in dataframe_object_adls.keys():\n",
    "            path = f'mnt/kevee/bronze/{v_project_name}/{table}.parquet'\n",
    "            df = dataframe_object[table]\n",
    "            df.write.mode('overwrite').parquet(path)\n",
    "            \n",
    "            # Store the dataframe in dataframe_object_adls so that future versions can be updated correctly\n",
    "            dataframe_object_adls[table] = df\n",
    "            \n",
    "        else:\n",
    "            df = dataframe_object_adls[table]\n",
    "            table_versions = df.select(\"modelVersionNumber\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "            \n",
    "            if model_version not in table_versions:\n",
    "                path = f'mnt/kevee/bronze/{v_project_name}/{table}.parquet'\n",
    "                dataframe_object_adls[table].write.mode('append').parquet(path)\n",
    "                \n",
    "            elif model_version in table_versions:\n",
    "                path = f'/mnt/kevee/bronze/{v_project_name}/{table}.parquet/'\n",
    "                # Load the existing data from the path\n",
    "                df_existing = spark.read.format(\"parquet\").load(path)\n",
    "                \n",
    "                # Select all rows with modelVersionNumber equal to model_version\n",
    "                df_updated = df_existing.filter(col(\"modelVersionNumber\") != model_version)\n",
    "                df_updated = df_updated.union(dataframe_object[table])\n",
    "                \n",
    "                # Overwrite the rows with modelVersionNumber equal to model_version\n",
    "                df_updated.write.mode('overwrite').parquet(path)\n",
    "                \n",
    "                # Update the dataframe_object_adls with the latest version\n",
    "                dataframe_object_adls[table] = df_updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4530c833-8d74-462f-bae9-a6b14739e015",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.notebook.exit(\"Success\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2_Loadingdata",
   "widgets": {
    "p_folder_id": {
     "currentValue": "urn:adsk.wipprod:fs.folder:co.Z1zGFWApS0Wjz8Kr7kGeTA",
     "nuid": "cb26ef58-379c-4787-8232-729e780e0a7c",
     "widgetInfo": {
      "defaultValue": "",
      "label": null,
      "name": "p_folder_id",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "p_model_name": {
     "currentValue": "01_07-Mx-TRA-X__-XX-000-X-XX.rvt",
     "nuid": "ff732ff7-3daa-409d-ab78-2276bca3f7ca",
     "widgetInfo": {
      "defaultValue": "",
      "label": null,
      "name": "p_model_name",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "p_project_id": {
     "currentValue": "d4d43adb-9f04-407e-a1aa-5e838276f25e",
     "nuid": "ef65162a-d136-4419-88ee-76ccd61b2ad6",
     "widgetInfo": {
      "defaultValue": "",
      "label": null,
      "name": "p_project_id",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "p_project_name": {
     "currentValue": "D1111",
     "nuid": "501f65f0-3e72-4835-ae49-0f9ca5df0421",
     "widgetInfo": {
      "defaultValue": "",
      "label": null,
      "name": "p_project_name",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "p_view_name": {
     "currentValue": "BIM360_D1111_Design Model_IFC Export View_Navisworks",
     "nuid": "db94f1ad-ec82-43bf-a864-8d0cf5a18fab",
     "widgetInfo": {
      "defaultValue": "",
      "label": null,
      "name": "p_view_name",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
